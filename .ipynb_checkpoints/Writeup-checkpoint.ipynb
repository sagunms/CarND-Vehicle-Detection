{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Vehicle Detection Project\n",
    "[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n",
    "\n",
    "Overview\n",
    "---\n",
    "\n",
    "This project detects and tracks vehicles using traditional computer vision and machine learning techniques. These include performing a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of vehicle/non-vehicle images and training a Linear Support Vector Machines (SVM) classifier model. The algorithm extracts features from the input video stream by applying a colour transform, performing HOG, Colour Histogram and Spatial binning. Then a sliding-window technique is used to scan the road for vehicles in the images by using the trained classifier to indicate that certain patches correspond to a vehicle or not. False positive are filtered out and vehicle tracking is stabalised by thresholding on heat maps over a number of frames of overlapping bounding boxes. \n",
    "\n",
    "[//]: # (Image References)\n",
    "[heatmaps]: ./output_images/heatmaps.png\n",
    "[hog_features_non_vehicles]: ./output_images/hog_features_non_vehicles.png\n",
    "[hog_features_vehicles]: ./output_images/hog_features_vehicles.png\n",
    "[noisy_classifier_detections]: ./output_images/noisy_classifier_detections.png\n",
    "[non_vehicle_images]: ./output_images/non_vehicle_images.png\n",
    "[output_bboxes]: ./output_images/output_bboxes.png\n",
    "[overview_gif]: ./output_images/overview.gif\n",
    "[overview_combined_gif]: ./output_images/overview_combined.gif\n",
    "[sliding_window_roi]: ./output_images/sliding_window_roi.png\n",
    "[vehicle_images]: ./output_images/vehicle_images.png\n",
    "[test_video]: ./annotated_project_video_test.mp4\n",
    "[final_video]: ./annotated_project_video.mp4\n",
    "\n",
    "The following animation demonstrate how the final model, combined with [Lane Detection](https://github.com/sagunms/CarND-Advanced-Lane-Lines), performs on the given video stream.\n",
    "\n",
    "![alt text][overview_combined_gif]\n",
    "\n",
    "Project goals\n",
    "---\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "Run Instructions\n",
    "---\n",
    "\n",
    "The project is written in python and utilises numpy, OpenCV, scikit learn and MoviePy.\n",
    "\n",
    "Here are the steps required to generate the model from scratch and run the project for vehicle tracking. \n",
    "\n",
    "#### Clone my project\n",
    "```bash\n",
    "git clone https://github.com/sagunms/CarND-Vehicle-Detection.git\n",
    "cd CarND-Vehicle-Detection\n",
    "```\n",
    "\n",
    "#### Activate conda environment\n",
    "Follow instructions from [CarND-Term1-Starter-Kit page](https://github.com/udacity/CarND-Term1-Starter-Kit) to setup the conda environment from scratch.\n",
    "```bash\n",
    "source activate carnd-term1\n",
    "```\n",
    "\n",
    "#### Download training data of vehicles and non-vehicles\n",
    "```bash\n",
    "mkdir data\n",
    "cd data\n",
    "wget https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/vehicles.zip\n",
    "wget https://s3.amazonaws.com/udacity-sdc/Vehicle_Tracking/non-vehicles.zip\n",
    "unzip vehicles.zip\n",
    "unzip non-vehicles.zip\n",
    "cd ..\n",
    "```\n",
    "\n",
    "#### Configure parameters\n",
    "```bash\n",
    "vim vehicle_lib/config.py\n",
    "```\n",
    "\n",
    "#### Train model from downloaded training data\n",
    "```bash\n",
    "python model.py -m model.mdl\n",
    "```\n",
    "\n",
    "#### Run vehicle detection project (output video)\n",
    "```bash\n",
    "python main.py -m model.mdl -i project_video.mp4 -o annotated_project_video.mp4\n",
    "```\n",
    "\n",
    "\n",
    "Project structure\n",
    "---\n",
    "\n",
    "### Source Code\n",
    "The code is divided up into several files which are imported by `model.py` and `main.py`.\n",
    "* `main.py` - Takes input video file, input trained model and outputs annotated video with bounding boxes highlighting the detected vehicles. Moreover, it also highlights the lane the vehicle is in due to the integration of lane detection library.\n",
    "* `model.py` - Reads vehicle and non-vehicle labelled images, extracts HOG features for both classes, splits features into training and validation datasets. Then, trains a pipeline consisting of StandardScaler and a Linear SVM classifier and saves the trained model as output.\n",
    "* `vehicle_lib/config.py` - Consists of configuration parameters such as colour space, HOG parameters, spatial size, histogram bins, sliding window parameters, and region of interest, etc.\n",
    "* `vehicle_lib/vehicle_detect.py` - Main class of the project which encapsulates sliding windows, feature generation, svm, remove duplicates and false positives, etc.\n",
    "* `vehicle_lib/feature_extract.py` - Consists of functions related to feature extraction such as HOG features, spatial binning, colour histogram, etc.\n",
    "* `vehicle_lib/window.py` - Consists of functions related to sliding window traversal and predicting which patch of the video frame contains a vehicle using the trained SVM model. \n",
    "* `vehicle_lib/heatmap.py` - Class for stablising detected heatmaps. Maintains history of heat maps over multiple frames and takes aggregate of all frames. \n",
    "* `vehicle_lib/utils.py` - Consists of utility functions to display images, features, traverse through subdirectories to load training images.\n",
    "* `vehicle_lib/debug.py` - Some plotting functions the helped during debugging.\n",
    "* `lane_lib/*` - Lane Detection library from my [Advanced Lane Finding](https://github.com/sagunms/CarND-Advanced-Lane-Lines) project.\n",
    "\n",
    "\n",
    "### Miscellaneous Files\n",
    " * `VehicleDetection.ipynb` - Jupyter notebook for generating various stages of the project to assist this writeup. Images produced from this notebook can also be found at `output_images/*.png`.\n",
    " * `model.mdl` - Trained model saved as the outcome of training the Linear SVM classifier from model.py. This file was then used in main.py to produce the annotated videos for demonstrating the working of my vehicle detection project. \n",
    " * `calib.p` - Pickle file containing instrinc camera calibration matrix and distortion coefficient saved as the outcome of `CameraCalibrate` class used during the initialisation of the lane detection pipeline.\n",
    " * `annotated_project_video.mp4` - The output of the vehicle detection project when processing against project_video.mp4 video stream.\n",
    " * `annotated_project_video_test.mp4` - The output of the vehicle detection project when processing against test_video.mp4 video stream. \n",
    " * `annotated_project_video_combined.mp4` - The output of the vehicle detection project when combined with lane finding project, and processing against `project_video.mp4` video stream. \n",
    "\n",
    "Algorithm\n",
    "---\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG) and other features\n",
    "\n",
    "First step is to extract the features used to train the classifier and then to classify the video frames.\n",
    "\n",
    "The code for this step is contained in `extract_features` function in `feature_extract.py`. This is invoked by `prepare_train_features` function in `model.py`, which is ultimately invoked when runing the `__main__` to train the model. \n",
    "\n",
    "I started by reading in all the `vehicle` and `non-vehicle` labelled images and calling `extract_features` function.  Here is an example of some of the `vehicle` and `non-vehicle` classes:\n",
    "\n",
    "![alt text][vehicle_images]\n",
    "\n",
    "![alt text][non_vehicle_images]\n",
    "\n",
    "After trying out different color spaces and different parameters `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`). I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like.\n",
    "\n",
    "First I tried different parameters however, the one provided in the course material was better and therefore settled with that. I used `YCrCb` color space and HOG parameters of `orientations=9`, `pix_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:\n",
    "\n",
    "![alt text][hog_features_vehicles] ![alt text][hog_features_non_vehicles]\n",
    "\n",
    "### Classifier\n",
    "\n",
    "After extracting features, we need to train a classifier to be able to differentiate between a portion of the frame as being a vehicle or non-vehicle. \n",
    "\n",
    "I used `sklearn.pipeline.Pipeline()` to encapsulate both StandardScalar and linear Support Vector Machine (SVM) into one, train it and save it as a model file using `sklearn.externals.joblib`. This help separate training and prediction into `model.py` and `main.py` files respectively.\n",
    "\n",
    "The `main.py` loads the saved model (`model.mdl`) and passes the loaded classifier pipeline to `VehicleDetect` class. \n",
    "\n",
    "The for training the classifier is contained in `extract_features` function in `feature_extract.py`. This is invoked by `prepare_train_features` function in `model.py`,which prepares vehicle and non-vehicle features from the provided training images and split into training and testing dataset at the ratio of 75% and 25% respectively. Initially, I experimented with various values of C parameter. However, I later found out that the default linear SVM parameters initialised achieved validation accuracy of 0.9903 which was sufficient for detecting vehicles from the video stream.\n",
    "\n",
    "The classifier pipleine parameters are as follows:\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('scaling', StandardScaler(copy=True, with_mean=0, with_std=1)), ('classifier', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False))])\n",
    "```\n",
    "\n",
    "### Sliding Window Search\n",
    "\n",
    "To locate the cars in each frame, a sliding window approach was used over a region of interest. Initially, I started with different window sizes and overlaps hoping to get a finer resolution and higher detection accuracy. I tried different window sizes, region of interest and overlaps. Through hit and trail, I settled for a more simplistic single scale window instead of varying the sizes. The parameters I used are as follows which can be found in `vehicle_lib/config.py`.\n",
    "\n",
    "* `xy_window = (96, 96)`\n",
    "* `xy_overlap = (.75, .75)`\n",
    "* `y_start_stop = [400, 600]`\n",
    "* `x_start_stop = [None, None]`\n",
    "\n",
    "The region of interest for sliding window search includes only the portion of the road,  spanning from left to right. \n",
    "\n",
    "![alt text][sliding_window_roi]\n",
    "\n",
    "For this project, I searched on YCrCb 3-channel HOG features plus spatially binned color and histograms of color in the feature vector, which provided a nice result. This was mostly based on recommended values available in computer vision literature. The parameters can be found in `vehicle_lib/config.py`. \n",
    "\n",
    "1. Spatial Binning\n",
    "    * `spatial_size = (32, 32)`\n",
    "    * Function: `bin_spatial()` in `vehicle_lib/feature_extract.py`.\n",
    "2. Color Histograms\n",
    "    * `hist_bins = 32`\n",
    "    * Function: `color_hist()` in `vehicle_lib/feature_extract.py`.\n",
    "3. Histogram of Oriented Gradients (HOG)\n",
    "    * `pix_per_cell = 8`\n",
    "    * `cell_per_block = 2`\n",
    "    * `orient = 9`\n",
    "    * `color_space = YCrCb`\n",
    "    * Function: `get_hog_features()` in `vehicle_lib/feature_extract.py`.\n",
    "    \n",
    "### Filtering False positives and Vehicle Tracking\n",
    "\n",
    "I recorded the positions of positive detections in each frame of the video. Numerous patches in the images are predicted as being a vehicle and therefore contains noisy false positives. \n",
    "\n",
    "![alt text][noisy_classifier_detections]\n",
    "\n",
    "The above figure illustrates the need to filter out overlapping bounding boxes by filtering. For this, from the positive detections I created a heatmap and then thresholded that map to identify vehicle positions. I then used `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap. I then assumed each blob corresponded to a vehicle. I constructed bounding boxes to cover the area of each blob detected. \n",
    "\n",
    "Here's an example result showing the heatmap from a video frame. \n",
    "\n",
    "![alt text][heatmaps]\n",
    "\n",
    "The bounding boxes then overlaid on the area of the blobs detected.\n",
    "\n",
    "![alt text][output_bboxes]\n",
    "\n",
    "This worked great for images but when testing with video frames, the bounding boxes fluctuated at different patches in the image. In order to achieve stable tracking of vehicles that were already detected in temporal dimension, I created a `StableHeatMaps` class in `vehicle_lib/heatmap.py` which maintains a historical sum of heat pixels (of same size as the input frame) over 20 frames. The class includes private methods `_add_heat()` which adds heat for all pixels that fall within the patch of positive detection by the classifier, and `_apply_threshold()` to remove false-positives by thresholding. \n",
    "\n",
    "The method `generate()` generates an aggregate sum of heatmap over history of 20 frames which thereby helps to stabalise the predicted bounding boxes. I am able to eliminate all false positives as shown in the project video, showing the method works fine.\n",
    "\n",
    "Video Implementation\n",
    "---\n",
    "\n",
    "The working implementation can be summarised with the following animation.\n",
    "\n",
    "![alt text][overview_gif]\n",
    "\n",
    "My pipeline was able to perform reasonably well on the entire [project video](./project_video.mp4). The working implementation after combining my previous [Advanced Lane Detection](https://github.com/sagunms/CarND-Advanced-Lane-Lines) project can be summarised with the following animation.\n",
    "\n",
    "![alt text][overview_combined_gif]\n",
    "\n",
    "Here's a link to [test video result](./annotated_project_video_test.mp4), [final project video result](./annotated_project_video.mp4), and [result of combined vehicle and lane detection](./annotated_project_video_combined.mp4).\n",
    "\n",
    "\n",
    "Discussion, Limitations and Improvements\n",
    "---\n",
    "\n",
    "This project was really exciting to work on but it's a shame I had very little time to work on it. The implementation is far from perfect, but vehicle detection works quite well for the given project video. However, several things could be improved.\n",
    "\n",
    "1. One of the main drawbacks is that my detection pipeline is very slow (~4.5s per frame) and therefore cannot be used for real-time applications. Recent deep learning and CNN techniques like YOLO seem better suited in terms of detection accuracy and real-time performance. Therefore it would be worth evaluating these modern methods as an alternative to traditional computer vision and machine learning techniques such as used in this project.\n",
    "2. My algorithm pipeline would probably fail in real-world scenarios. For example, it would fail for objects different from vehicles/non-vehicles it was trained with such as motorbikes, cyclists and pedestrians. Perhaps if the training images from the same camera is used, the classifier accuracy would be better.\n",
    "3. Similar to advanced line detection limitations, there can be some false positives produced in cases such as shadow regions.\n",
    "4. This was tested for only one video and therefore, it there can be some false positives produced in cases such as shadow regions. \n",
    "5. My implementation sums heat map over several historical frames and then thresholding eliminate false-positives and stabilise tracking of vehicles quite well. However, I would be inclined to explore a more robust approaches such as Kalman Filters for vehicle tracking. \n",
    "6. A simple method of improving processing speed would be to drop frames or scan frames at high frequency over high confidence heatmaps and lower at other reason. Kalman filter again, would be better in tracking with lower computation.\n",
    "7. Had there been sufficient time, I would integrate Advanced Lane Lines detection into this project. It would also be interesting to integrate an additional pipeline for using Traffic line classification to detect road signs. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
